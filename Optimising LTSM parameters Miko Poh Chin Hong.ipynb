{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1987-12-28</th>\n",
       "      <td>824.400024</td>\n",
       "      <td>824.400024</td>\n",
       "      <td>824.400024</td>\n",
       "      <td>824.400024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-12-29</th>\n",
       "      <td>810.900024</td>\n",
       "      <td>810.900024</td>\n",
       "      <td>810.900024</td>\n",
       "      <td>810.900024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-12-30</th>\n",
       "      <td>823.200012</td>\n",
       "      <td>823.200012</td>\n",
       "      <td>823.200012</td>\n",
       "      <td>823.200012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-01-04</th>\n",
       "      <td>833.599976</td>\n",
       "      <td>833.599976</td>\n",
       "      <td>833.599976</td>\n",
       "      <td>833.599976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988-01-05</th>\n",
       "      <td>879.299988</td>\n",
       "      <td>879.299988</td>\n",
       "      <td>879.299988</td>\n",
       "      <td>879.299988</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-14</th>\n",
       "      <td>3280.439941</td>\n",
       "      <td>3293.469971</td>\n",
       "      <td>3274.959961</td>\n",
       "      <td>3278.570068</td>\n",
       "      <td>271342400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-15</th>\n",
       "      <td>3280.949951</td>\n",
       "      <td>3287.530029</td>\n",
       "      <td>3256.790039</td>\n",
       "      <td>3273.750000</td>\n",
       "      <td>252158300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-16</th>\n",
       "      <td>3253.669922</td>\n",
       "      <td>3272.120117</td>\n",
       "      <td>3237.790039</td>\n",
       "      <td>3240.810059</td>\n",
       "      <td>387112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-19</th>\n",
       "      <td>3239.500000</td>\n",
       "      <td>3273.350098</td>\n",
       "      <td>3239.500000</td>\n",
       "      <td>3256.610107</td>\n",
       "      <td>172547000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-20</th>\n",
       "      <td>3246.300049</td>\n",
       "      <td>3262.370117</td>\n",
       "      <td>3237.719971</td>\n",
       "      <td>3253.969971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8737 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close     Volume\n",
       "Date                                                                     \n",
       "1987-12-28   824.400024   824.400024   824.400024   824.400024          0\n",
       "1987-12-29   810.900024   810.900024   810.900024   810.900024          0\n",
       "1987-12-30   823.200012   823.200012   823.200012   823.200012          0\n",
       "1988-01-04   833.599976   833.599976   833.599976   833.599976          0\n",
       "1988-01-05   879.299988   879.299988   879.299988   879.299988          0\n",
       "...                 ...          ...          ...          ...        ...\n",
       "2022-12-14  3280.439941  3293.469971  3274.959961  3278.570068  271342400\n",
       "2022-12-15  3280.949951  3287.530029  3256.790039  3273.750000  252158300\n",
       "2022-12-16  3253.669922  3272.120117  3237.790039  3240.810059  387112800\n",
       "2022-12-19  3239.500000  3273.350098  3239.500000  3256.610107  172547000\n",
       "2022-12-20  3246.300049  3262.370117  3237.719971  3253.969971          0\n",
       "\n",
       "[8737 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading STI's history into a dataframe\n",
    "sti= yf.Ticker(\"^sti\")\n",
    "sti_df = sti.history(period='max')\n",
    "sti_df.drop([\"Dividends\",\"Stock Splits\"], axis=1, inplace=True)\n",
    "sti_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick up Close column as array for data before 2021\n",
    "training_set = sti_df[sti_df.index.year<2021][[\"Close\"]].values # Array need to be in 2D\n",
    "testing_set = sti_df[sti_df.index.year>=2021][[\"Close\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform scaling to trasform data between zero and one\n",
    "sc = MinMaxScaler()\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "testing_set_scaled = sc.transform(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure input to have a certain a amount of steps to one output\n",
    "X_train_25 = []\n",
    "y_train_25 = []\n",
    "X_test_25 = []\n",
    "y_test_25 = []\n",
    "\n",
    "# 100 steps for one input, about 1 month for 1 value for train set\n",
    "for i in range(25, len(training_set_scaled)):\n",
    "    X_train_25.append(training_set_scaled[i-25:i, 0]) # Starting from 0 row to 100th row\n",
    "    y_train_25.append(training_set_scaled[i,0]) # From 100th row onwards\n",
    "X_train_25, y_train_25 = np.array(X_train_25), np.array(y_train_25)\n",
    "\n",
    "# 100 steps for one input, about 1 month for 1 value for test set\n",
    "testing_set_scaled_25 = np.vstack((training_set_scaled[-25:], testing_set_scaled))\n",
    "for i in range(25, len(testing_set_scaled_25)):\n",
    "    X_test_25.append(testing_set_scaled_25[i-25:i, 0])\n",
    "    y_test_25.append(testing_set_scaled_25[i,0])\n",
    "X_test_25, y_test_25 = np.array(X_test_25), np.array(y_test_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure input to have a certain a amount of steps to one output\n",
    "X_train_50 = []\n",
    "y_train_50 = []\n",
    "X_test_50 = []\n",
    "y_test_50 = []\n",
    "\n",
    "# 100 steps for one input, about 2 month for 1 value for train set\n",
    "for i in range(50, len(training_set_scaled)):\n",
    "    X_train_50.append(training_set_scaled[i-50:i, 0]) # Starting from 0 row to 100th row\n",
    "    y_train_50.append(training_set_scaled[i,0]) # From 100th row onwards\n",
    "X_train_50, y_train_50 = np.array(X_train_50), np.array(y_train_50)\n",
    "\n",
    "# 50 steps for one input, about 2 month for 1 value for test set\n",
    "testing_set_scaled_50 = np.vstack((training_set_scaled[-50:], testing_set_scaled))\n",
    "for i in range(50, len(testing_set_scaled_50)):\n",
    "    X_test_50.append(testing_set_scaled_50[i-50:i, 0])\n",
    "    y_test_50.append(testing_set_scaled_50[i,0])\n",
    "X_test_50, y_test_50 = np.array(X_test_50), np.array(y_test_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure input to have a certain a amount of steps to one output\n",
    "X_train_100 = []\n",
    "y_train_100 = []\n",
    "X_test_100 = []\n",
    "y_test_100 = []\n",
    "\n",
    "# 100 steps for one input, about 5 month for 1 value for train set\n",
    "for i in range(100, len(training_set_scaled)):\n",
    "    X_train_100.append(training_set_scaled[i-100:i, 0]) # Starting from 0 row to 100th row\n",
    "    y_train_100.append(training_set_scaled[i,0]) # From 100th row onwards\n",
    "X_train_100, y_train_100 = np.array(X_train_100), np.array(y_train_100)\n",
    "\n",
    "# 100 steps for one input, about 5 month for 1 value for test set\n",
    "testing_set_scaled_100 = np.vstack((training_set_scaled[-100:], testing_set_scaled))\n",
    "for i in range(100, len(testing_set_scaled_100)):\n",
    "    X_test_100.append(testing_set_scaled_100[i-100:i, 0])\n",
    "    y_test_100.append(testing_set_scaled_100[i,0])\n",
    "X_test_100, y_test_100 = np.array(X_test_100), np.array(y_test_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure input to have a certain a amount of steps to one output\n",
    "X_train_200 = []\n",
    "y_train_200 = []\n",
    "X_test_200 = []\n",
    "y_test_200 = []\n",
    "\n",
    "# 200 steps for one input, about 5 month for 1 value for train set\n",
    "for i in range(200, len(training_set_scaled)):\n",
    "    X_train_200.append(training_set_scaled[i-200:i, 0]) # Starting from 0 row to 100th row\n",
    "    y_train_200.append(training_set_scaled[i,0]) # From 100th row onwards\n",
    "X_train_200, y_train_200 = np.array(X_train_200), np.array(y_train_200)\n",
    "\n",
    "# 100 steps for one input, about 5 month for 1 value for test set\n",
    "testing_set_scaled_200 = np.vstack((training_set_scaled[-200:], testing_set_scaled))\n",
    "for i in range(200, len(testing_set_scaled_200)):\n",
    "    X_test_200.append(testing_set_scaled_200[i-200:i, 0])\n",
    "    y_test_200.append(testing_set_scaled_200[i,0])\n",
    "X_test_200, y_test_200 = np.array(X_test_200), np.array(y_test_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape X_train data into 3D array as RNN only accept 3D data\n",
    "# Shape(number of records, number of steps, 1)\n",
    "X_train_25 = np.reshape(X_train_25, (X_train_25.shape[0], X_train_25.shape[1], 1))\n",
    "X_train_50 = np.reshape(X_train_50, (X_train_50.shape[0], X_train_50.shape[1], 1))\n",
    "X_train_100 = np.reshape(X_train_100, (X_train_100.shape[0], X_train_100.shape[1], 1))\n",
    "X_train_200 = np.reshape(X_train_200, (X_train_200.shape[0], X_train_200.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout # Stopping a certain amount of neurons to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = [X_train_25, X_train_50, X_train_100, X_train_200]\n",
    "y_train_list = [y_train_25, y_train_50, y_train_100, y_train_200]\n",
    "x_test_list = [X_test_25, X_test_50, X_test_100, X_test_200]\n",
    "y_test_list = [y_test_25, y_test_50, y_test_100, y_test_200]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out different parameters to find the best model\n",
    "# NOTE: It takes more than 10 hours to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "206/206 [==============================] - 10s 29ms/step - loss: 0.0062 - val_loss: 8.6749e-04\n",
      "Epoch 2/100\n",
      "206/206 [==============================] - 4s 19ms/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 3/100\n",
      "206/206 [==============================] - 5s 22ms/step - loss: 0.0020 - val_loss: 6.3492e-04\n",
      "Epoch 4/100\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 0.0019 - val_loss: 8.3461e-04\n",
      "Epoch 5/100\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 0.0017 - val_loss: 5.2650e-04\n",
      "Epoch 6/100\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 0.0017 - val_loss: 5.0860e-04\n",
      "Epoch 7/100\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 0.0014 - val_loss: 5.0635e-04\n",
      "Epoch 8/100\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 0.0014 - val_loss: 4.2575e-04\n",
      "Epoch 9/100\n",
      "206/206 [==============================] - 5s 25ms/step - loss: 0.0013 - val_loss: 3.7790e-04\n",
      "Epoch 10/100\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 0.0011 - val_loss: 5.2574e-04\n",
      "Epoch 11/100\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 0.0010 - val_loss: 3.8048e-04\n",
      "Epoch 12/100\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 9.7901e-04 - val_loss: 4.1250e-04\n",
      "Epoch 13/100\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 9.3315e-04 - val_loss: 5.9960e-04\n",
      "Epoch 14/100\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 8.7672e-04 - val_loss: 0.0010\n",
      "Epoch 15/100\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 8.4616e-04 - val_loss: 4.1397e-04\n",
      "Epoch 16/100\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 7.9955e-04 - val_loss: 4.1724e-04\n",
      "Epoch 17/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 7.8033e-04 - val_loss: 6.6071e-04\n",
      "Epoch 18/100\n",
      "206/206 [==============================] - 5s 27ms/step - loss: 7.3385e-04 - val_loss: 2.4701e-04\n",
      "Epoch 19/100\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 6.9411e-04 - val_loss: 0.0015\n",
      "Epoch 20/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 7.0877e-04 - val_loss: 5.8597e-04\n",
      "Epoch 21/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 6.6573e-04 - val_loss: 1.9566e-04\n",
      "Epoch 22/100\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 6.8647e-04 - val_loss: 3.2491e-04\n",
      "Epoch 23/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 6.4706e-04 - val_loss: 2.9336e-04\n",
      "Epoch 24/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 6.4164e-04 - val_loss: 2.0961e-04\n",
      "Epoch 25/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.8975e-04 - val_loss: 1.8771e-04\n",
      "Epoch 26/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 6.2871e-04 - val_loss: 2.3684e-04\n",
      "Epoch 27/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 6.2771e-04 - val_loss: 1.7180e-04\n",
      "Epoch 28/100\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 6.0415e-04 - val_loss: 1.6183e-04\n",
      "Epoch 29/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.8532e-04 - val_loss: 1.9024e-04\n",
      "Epoch 30/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.6171e-04 - val_loss: 1.6224e-04\n",
      "Epoch 31/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.4763e-04 - val_loss: 1.6137e-04\n",
      "Epoch 32/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.6612e-04 - val_loss: 2.3335e-04\n",
      "Epoch 33/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.5165e-04 - val_loss: 3.8813e-04\n",
      "Epoch 34/100\n",
      "206/206 [==============================] - 6s 28ms/step - loss: 5.4942e-04 - val_loss: 1.6891e-04\n",
      "Epoch 35/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.5494e-04 - val_loss: 4.8190e-04\n",
      "Epoch 36/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.8206e-04 - val_loss: 2.4878e-04\n",
      "Epoch 37/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.6772e-04 - val_loss: 1.3541e-04\n",
      "Epoch 38/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.3532e-04 - val_loss: 3.3070e-04\n",
      "Epoch 39/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.2586e-04 - val_loss: 2.5338e-04\n",
      "Epoch 40/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.4873e-04 - val_loss: 2.3181e-04\n",
      "Epoch 41/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.8056e-04 - val_loss: 1.5491e-04\n",
      "Epoch 42/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.3282e-04 - val_loss: 1.1776e-04\n",
      "Epoch 43/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.3027e-04 - val_loss: 1.6130e-04\n",
      "Epoch 44/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.9138e-04 - val_loss: 1.3747e-04\n",
      "Epoch 45/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 5.2390e-04 - val_loss: 2.9972e-04\n",
      "Epoch 46/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.2509e-04 - val_loss: 2.0892e-04\n",
      "Epoch 47/100\n",
      "206/206 [==============================] - 7s 32ms/step - loss: 5.4819e-04 - val_loss: 1.1173e-04\n",
      "Epoch 48/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.1412e-04 - val_loss: 8.7210e-04\n",
      "Epoch 49/100\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 5.1406e-04 - val_loss: 1.8153e-04\n",
      "Epoch 50/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.5037e-04 - val_loss: 3.5580e-04\n",
      "Epoch 51/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.0766e-04 - val_loss: 1.2431e-04\n",
      "Epoch 52/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.1476e-04 - val_loss: 3.4494e-04\n",
      "Epoch 53/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.2253e-04 - val_loss: 1.1114e-04\n",
      "Epoch 54/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.4801e-04 - val_loss: 3.1432e-04\n",
      "Epoch 55/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.1435e-04 - val_loss: 1.6972e-04\n",
      "Epoch 56/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.9259e-04 - val_loss: 1.0994e-04\n",
      "Epoch 57/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.9455e-04 - val_loss: 1.2228e-04\n",
      "Epoch 58/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.0402e-04 - val_loss: 1.4494e-04\n",
      "Epoch 59/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.0941e-04 - val_loss: 1.6201e-04\n",
      "Epoch 60/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.7661e-04 - val_loss: 2.9535e-04\n",
      "Epoch 61/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 5.0127e-04 - val_loss: 1.1581e-04\n",
      "Epoch 62/100\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.8629e-04 - val_loss: 1.3943e-04\n",
      "Epoch 63/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.8853e-04 - val_loss: 1.1908e-04\n",
      "Epoch 64/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.8163e-04 - val_loss: 1.3158e-04\n",
      "Epoch 65/100\n",
      "206/206 [==============================] - 6s 29ms/step - loss: 4.7652e-04 - val_loss: 1.1357e-04\n",
      "Epoch 66/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.8240e-04 - val_loss: 9.9732e-05\n",
      "Epoch 67/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.9247e-04 - val_loss: 6.3240e-04\n",
      "Epoch 68/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.8327e-04 - val_loss: 1.1561e-04\n",
      "Epoch 69/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.6328e-04 - val_loss: 1.5572e-04\n",
      "Epoch 70/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.7552e-04 - val_loss: 1.1771e-04\n",
      "Epoch 71/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.8416e-04 - val_loss: 1.1797e-04\n",
      "Epoch 72/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.8149e-04 - val_loss: 1.2116e-04\n",
      "Epoch 73/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.7042e-04 - val_loss: 9.7483e-05\n",
      "Epoch 74/100\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.5679e-04 - val_loss: 1.1281e-04\n",
      "Epoch 75/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.6985e-04 - val_loss: 2.8750e-04\n",
      "Epoch 76/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.3886e-04 - val_loss: 1.0321e-04\n",
      "Epoch 77/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.7170e-04 - val_loss: 2.3916e-04\n",
      "Epoch 78/100\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.5928e-04 - val_loss: 4.0500e-04\n",
      "Epoch 79/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.9978e-04 - val_loss: 1.6049e-04\n",
      "Epoch 80/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.4345e-04 - val_loss: 1.9282e-04\n",
      "Epoch 81/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.6144e-04 - val_loss: 3.0674e-04\n",
      "Epoch 82/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.6564e-04 - val_loss: 1.0508e-04\n",
      "Epoch 83/100\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.8966e-04 - val_loss: 1.8063e-04\n",
      "Epoch 84/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.5362e-04 - val_loss: 1.1926e-04\n",
      "Epoch 85/100\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.3560e-04 - val_loss: 4.6889e-04\n",
      "Epoch 86/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.9918e-04 - val_loss: 1.2938e-04\n",
      "Epoch 87/100\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.6849e-04 - val_loss: 2.3880e-04\n",
      "Epoch 88/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.4826e-04 - val_loss: 9.5204e-05\n",
      "Epoch 89/100\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.7258e-04 - val_loss: 1.2036e-04\n",
      "Epoch 90/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.3208e-04 - val_loss: 9.8469e-05\n",
      "Epoch 91/100\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.6195e-04 - val_loss: 1.2645e-04\n",
      "Epoch 92/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.7765e-04 - val_loss: 2.1681e-04\n",
      "Epoch 93/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.5421e-04 - val_loss: 9.4737e-05\n",
      "Epoch 94/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.4088e-04 - val_loss: 2.6748e-04\n",
      "Epoch 95/100\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.8855e-04 - val_loss: 1.1657e-04\n",
      "Epoch 96/100\n",
      "206/206 [==============================] - 6s 30ms/step - loss: 4.7787e-04 - val_loss: 1.8381e-04\n",
      "Epoch 97/100\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.8984e-04 - val_loss: 4.8888e-04\n",
      "Epoch 98/100\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.6024e-04 - val_loss: 1.9324e-04\n",
      "Epoch 99/100\n",
      "206/206 [==============================] - 6s 31ms/step - loss: 4.7391e-04 - val_loss: 1.3521e-04\n",
      "Epoch 100/100\n",
      "206/206 [==============================] - 6s 32ms/step - loss: 4.2755e-04 - val_loss: 1.0999e-04\n",
      "16/16 [==============================] - 3s 12ms/step\n",
      "For train: [[[0.0063047 ]\n",
      "  [0.00190836]\n",
      "  [0.00591391]\n",
      "  ...\n",
      "  [0.03382259]\n",
      "  [0.02831901]\n",
      "  [0.02708152]]\n",
      "\n",
      " [[0.00190836]\n",
      "  [0.00591391]\n",
      "  [0.00930072]\n",
      "  ...\n",
      "  [0.02831901]\n",
      "  [0.02708152]\n",
      "  [0.0294588 ]]\n",
      "\n",
      " [[0.00591391]\n",
      "  [0.00930072]\n",
      "  [0.02418318]\n",
      "  ...\n",
      "  [0.02708152]\n",
      "  [0.0294588 ]\n",
      "  [0.02509501]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.6655551 ]\n",
      "  [0.67950939]\n",
      "  [0.67231897]\n",
      "  ...\n",
      "  [0.66054648]\n",
      "  [0.66336019]\n",
      "  [0.6627414 ]]\n",
      "\n",
      " [[0.67950939]\n",
      "  [0.67231897]\n",
      "  [0.66838829]\n",
      "  ...\n",
      "  [0.66336019]\n",
      "  [0.6627414 ]\n",
      "  [0.66534664]]\n",
      "\n",
      " [[0.67231897]\n",
      "  [0.66838829]\n",
      "  [0.66784773]\n",
      "  ...\n",
      "  [0.6627414 ]\n",
      "  [0.66534664]\n",
      "  [0.67221148]]], the MSE is 9.2294419612906e-05\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 27s 66ms/step - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 3/100\n",
      "205/205 [==============================] - 11s 52ms/step - loss: 0.0022 - val_loss: 9.4745e-04\n",
      "Epoch 4/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 0.0021 - val_loss: 5.8941e-04\n",
      "Epoch 5/100\n",
      "205/205 [==============================] - 10s 50ms/step - loss: 0.0019 - val_loss: 8.2903e-04\n",
      "Epoch 6/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 7/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 0.0014 - val_loss: 8.1166e-04\n",
      "Epoch 8/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 9/100\n",
      "205/205 [==============================] - 10s 50ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 10/100\n",
      "205/205 [==============================] - 10s 50ms/step - loss: 0.0011 - val_loss: 3.8048e-04\n",
      "Epoch 11/100\n",
      "205/205 [==============================] - 10s 50ms/step - loss: 0.0011 - val_loss: 3.3445e-04\n",
      "Epoch 12/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 0.0010 - val_loss: 3.1525e-04\n",
      "Epoch 13/100\n",
      "205/205 [==============================] - 10s 50ms/step - loss: 9.6181e-04 - val_loss: 5.2025e-04\n",
      "Epoch 14/100\n",
      "205/205 [==============================] - 11s 51ms/step - loss: 9.4858e-04 - val_loss: 3.5365e-04\n",
      "Epoch 15/100\n",
      "205/205 [==============================] - 10s 50ms/step - loss: 8.7545e-04 - val_loss: 2.9819e-04\n",
      "Epoch 16/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 7.8550e-04 - val_loss: 3.1774e-04\n",
      "Epoch 17/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 7.9942e-04 - val_loss: 2.4405e-04\n",
      "Epoch 18/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 7.9335e-04 - val_loss: 2.4942e-04\n",
      "Epoch 19/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 7.0393e-04 - val_loss: 3.0781e-04\n",
      "Epoch 20/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 7.3476e-04 - val_loss: 2.2170e-04\n",
      "Epoch 21/100\n",
      "205/205 [==============================] - 11s 51ms/step - loss: 6.9081e-04 - val_loss: 2.4658e-04\n",
      "Epoch 22/100\n",
      "205/205 [==============================] - 11s 52ms/step - loss: 6.3885e-04 - val_loss: 4.5197e-04\n",
      "Epoch 23/100\n",
      "205/205 [==============================] - 11s 51ms/step - loss: 6.8900e-04 - val_loss: 2.6800e-04\n",
      "Epoch 24/100\n",
      "205/205 [==============================] - 11s 51ms/step - loss: 6.1069e-04 - val_loss: 1.8685e-04\n",
      "Epoch 25/100\n",
      "205/205 [==============================] - 11s 52ms/step - loss: 6.3414e-04 - val_loss: 0.0010\n",
      "Epoch 26/100\n",
      "205/205 [==============================] - 11s 51ms/step - loss: 6.1029e-04 - val_loss: 2.5967e-04\n",
      "Epoch 27/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 6.6512e-04 - val_loss: 2.3665e-04\n",
      "Epoch 28/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 6.1612e-04 - val_loss: 5.1067e-04\n",
      "Epoch 29/100\n",
      "205/205 [==============================] - 11s 52ms/step - loss: 5.9755e-04 - val_loss: 2.6372e-04\n",
      "Epoch 30/100\n",
      "205/205 [==============================] - 11s 52ms/step - loss: 5.6043e-04 - val_loss: 1.4730e-04\n",
      "Epoch 31/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 5.4814e-04 - val_loss: 6.0551e-04\n",
      "Epoch 32/100\n",
      "205/205 [==============================] - 11s 52ms/step - loss: 5.8476e-04 - val_loss: 3.1709e-04\n",
      "Epoch 33/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 5.4688e-04 - val_loss: 5.6491e-04\n",
      "Epoch 34/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 5.3470e-04 - val_loss: 5.4747e-04\n",
      "Epoch 35/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 5.5203e-04 - val_loss: 2.1081e-04\n",
      "Epoch 36/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 5.2816e-04 - val_loss: 3.5645e-04\n",
      "Epoch 37/100\n",
      "205/205 [==============================] - 11s 52ms/step - loss: 5.2861e-04 - val_loss: 9.7880e-04\n",
      "Epoch 38/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 5.3456e-04 - val_loss: 1.6287e-04\n",
      "Epoch 39/100\n",
      "205/205 [==============================] - 11s 51ms/step - loss: 5.4811e-04 - val_loss: 3.4745e-04\n",
      "Epoch 40/100\n",
      "205/205 [==============================] - 11s 51ms/step - loss: 5.5007e-04 - val_loss: 1.1397e-04\n",
      "Epoch 41/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 5.2338e-04 - val_loss: 1.8756e-04\n",
      "Epoch 42/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 5.1176e-04 - val_loss: 1.4807e-04\n",
      "Epoch 43/100\n",
      "205/205 [==============================] - 10s 47ms/step - loss: 5.1811e-04 - val_loss: 8.9162e-04\n",
      "Epoch 44/100\n",
      "205/205 [==============================] - 10s 47ms/step - loss: 5.4299e-04 - val_loss: 1.5882e-04\n",
      "Epoch 45/100\n",
      "205/205 [==============================] - 10s 48ms/step - loss: 5.0494e-04 - val_loss: 2.4907e-04\n",
      "Epoch 46/100\n",
      "205/205 [==============================] - 11s 51ms/step - loss: 5.3641e-04 - val_loss: 1.3490e-04\n",
      "Epoch 47/100\n",
      "205/205 [==============================] - 10s 50ms/step - loss: 5.0481e-04 - val_loss: 2.1790e-04\n",
      "Epoch 48/100\n",
      "205/205 [==============================] - 11s 51ms/step - loss: 4.8159e-04 - val_loss: 1.2597e-04\n",
      "Epoch 49/100\n",
      "205/205 [==============================] - 11s 52ms/step - loss: 5.1728e-04 - val_loss: 6.6298e-04\n",
      "Epoch 50/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 5.4829e-04 - val_loss: 4.4699e-04\n",
      "Epoch 51/100\n",
      "205/205 [==============================] - 9s 42ms/step - loss: 5.2270e-04 - val_loss: 1.3025e-04\n",
      "Epoch 52/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 5.2733e-04 - val_loss: 1.0498e-04\n",
      "Epoch 53/100\n",
      "205/205 [==============================] - 11s 51ms/step - loss: 4.7545e-04 - val_loss: 1.0842e-04\n",
      "Epoch 54/100\n",
      "205/205 [==============================] - 10s 51ms/step - loss: 5.0290e-04 - val_loss: 1.7516e-04\n",
      "Epoch 55/100\n",
      "205/205 [==============================] - 11s 52ms/step - loss: 4.9711e-04 - val_loss: 9.9131e-05\n",
      "Epoch 56/100\n",
      "205/205 [==============================] - 11s 53ms/step - loss: 4.7917e-04 - val_loss: 1.2156e-04\n",
      "Epoch 57/100\n",
      "205/205 [==============================] - 11s 54ms/step - loss: 4.5851e-04 - val_loss: 1.1904e-04\n",
      "Epoch 58/100\n",
      "205/205 [==============================] - 11s 53ms/step - loss: 4.6245e-04 - val_loss: 9.1246e-05\n",
      "Epoch 59/100\n",
      "205/205 [==============================] - 11s 52ms/step - loss: 4.8816e-04 - val_loss: 4.4277e-04\n",
      "Epoch 60/100\n",
      "205/205 [==============================] - 11s 53ms/step - loss: 4.9577e-04 - val_loss: 1.1747e-04\n",
      "Epoch 61/100\n",
      "205/205 [==============================] - 11s 53ms/step - loss: 4.6965e-04 - val_loss: 1.1349e-04\n",
      "Epoch 62/100\n",
      "205/205 [==============================] - 11s 52ms/step - loss: 4.7624e-04 - val_loss: 1.5876e-04\n",
      "Epoch 63/100\n",
      "205/205 [==============================] - 11s 53ms/step - loss: 4.6323e-04 - val_loss: 1.1852e-04\n",
      "Epoch 64/100\n",
      "205/205 [==============================] - 11s 52ms/step - loss: 4.6753e-04 - val_loss: 1.0087e-04\n",
      "Epoch 65/100\n",
      "205/205 [==============================] - 11s 54ms/step - loss: 4.7777e-04 - val_loss: 1.0245e-04\n",
      "Epoch 66/100\n",
      "205/205 [==============================] - 11s 53ms/step - loss: 4.7041e-04 - val_loss: 2.6691e-04\n",
      "Epoch 67/100\n",
      "205/205 [==============================] - 11s 53ms/step - loss: 4.6349e-04 - val_loss: 1.6792e-04\n",
      "Epoch 68/100\n",
      "205/205 [==============================] - 11s 53ms/step - loss: 4.3598e-04 - val_loss: 2.1021e-04\n",
      "Epoch 69/100\n",
      "205/205 [==============================] - 11s 53ms/step - loss: 4.4923e-04 - val_loss: 2.6441e-04\n",
      "Epoch 70/100\n",
      "205/205 [==============================] - 11s 53ms/step - loss: 4.4002e-04 - val_loss: 1.8298e-04\n",
      "Epoch 71/100\n",
      "205/205 [==============================] - 11s 53ms/step - loss: 4.6493e-04 - val_loss: 8.7589e-05\n",
      "Epoch 72/100\n",
      "205/205 [==============================] - 11s 54ms/step - loss: 4.3855e-04 - val_loss: 9.9723e-05\n",
      "Epoch 73/100\n",
      "205/205 [==============================] - 11s 54ms/step - loss: 4.5927e-04 - val_loss: 1.8208e-04\n",
      "Epoch 74/100\n",
      "205/205 [==============================] - 13s 65ms/step - loss: 4.4437e-04 - val_loss: 5.7265e-04\n",
      "Epoch 75/100\n",
      "205/205 [==============================] - 12s 57ms/step - loss: 4.7052e-04 - val_loss: 9.0935e-05\n",
      "Epoch 76/100\n",
      "205/205 [==============================] - 12s 60ms/step - loss: 4.4758e-04 - val_loss: 3.6190e-04\n",
      "Epoch 77/100\n",
      "205/205 [==============================] - 13s 61ms/step - loss: 4.5510e-04 - val_loss: 8.5180e-05\n",
      "Epoch 78/100\n",
      "205/205 [==============================] - 12s 58ms/step - loss: 4.7380e-04 - val_loss: 8.6676e-05\n",
      "Epoch 79/100\n",
      "205/205 [==============================] - 12s 57ms/step - loss: 4.4265e-04 - val_loss: 1.1602e-04\n",
      "Epoch 80/100\n",
      "205/205 [==============================] - 11s 56ms/step - loss: 4.4057e-04 - val_loss: 4.1122e-04\n",
      "Epoch 81/100\n",
      "205/205 [==============================] - 12s 57ms/step - loss: 4.6016e-04 - val_loss: 8.0915e-05\n",
      "Epoch 82/100\n",
      "205/205 [==============================] - 12s 59ms/step - loss: 4.5846e-04 - val_loss: 9.7187e-05\n",
      "Epoch 83/100\n",
      "205/205 [==============================] - 12s 57ms/step - loss: 4.2370e-04 - val_loss: 9.6610e-05\n",
      "Epoch 84/100\n",
      "205/205 [==============================] - 11s 55ms/step - loss: 4.3322e-04 - val_loss: 2.1447e-04\n",
      "Epoch 85/100\n",
      "205/205 [==============================] - 13s 63ms/step - loss: 4.4258e-04 - val_loss: 9.6758e-05\n",
      "Epoch 86/100\n",
      "205/205 [==============================] - 11s 56ms/step - loss: 4.3566e-04 - val_loss: 4.6919e-04\n",
      "Epoch 87/100\n",
      "205/205 [==============================] - 12s 60ms/step - loss: 4.3128e-04 - val_loss: 2.4956e-04\n",
      "Epoch 88/100\n",
      "205/205 [==============================] - 12s 58ms/step - loss: 4.4864e-04 - val_loss: 5.8204e-04\n",
      "Epoch 89/100\n",
      "205/205 [==============================] - 12s 57ms/step - loss: 4.2515e-04 - val_loss: 1.0968e-04\n",
      "Epoch 90/100\n",
      "205/205 [==============================] - 12s 60ms/step - loss: 4.2129e-04 - val_loss: 1.1212e-04\n",
      "Epoch 91/100\n",
      "205/205 [==============================] - 11s 55ms/step - loss: 4.7030e-04 - val_loss: 8.6100e-05\n",
      "Epoch 92/100\n",
      "205/205 [==============================] - 12s 58ms/step - loss: 4.2456e-04 - val_loss: 1.2197e-04\n",
      "Epoch 93/100\n",
      "205/205 [==============================] - 12s 58ms/step - loss: 4.2270e-04 - val_loss: 1.0885e-04\n",
      "Epoch 94/100\n",
      "205/205 [==============================] - 11s 55ms/step - loss: 4.1672e-04 - val_loss: 1.4265e-04\n",
      "Epoch 95/100\n",
      "205/205 [==============================] - 12s 59ms/step - loss: 4.3136e-04 - val_loss: 3.5056e-04\n",
      "Epoch 96/100\n",
      "205/205 [==============================] - 13s 61ms/step - loss: 4.5046e-04 - val_loss: 1.1119e-04\n",
      "Epoch 97/100\n",
      "205/205 [==============================] - 12s 59ms/step - loss: 4.3380e-04 - val_loss: 1.9316e-04\n",
      "Epoch 98/100\n",
      "205/205 [==============================] - 11s 55ms/step - loss: 4.2924e-04 - val_loss: 1.7722e-04\n",
      "Epoch 99/100\n",
      "205/205 [==============================] - 12s 60ms/step - loss: 4.3176e-04 - val_loss: 1.1512e-04\n",
      "Epoch 100/100\n",
      "205/205 [==============================] - 12s 59ms/step - loss: 4.2012e-04 - val_loss: 8.0391e-05\n",
      "16/16 [==============================] - 4s 22ms/step\n",
      "For train: [[[0.0063047 ]\n",
      "  [0.00190836]\n",
      "  [0.00591391]\n",
      "  ...\n",
      "  [0.03665578]\n",
      "  [0.04144292]\n",
      "  [0.04342943]]\n",
      "\n",
      " [[0.00190836]\n",
      "  [0.00591391]\n",
      "  [0.00930072]\n",
      "  ...\n",
      "  [0.04144292]\n",
      "  [0.04342943]\n",
      "  [0.04414585]]\n",
      "\n",
      " [[0.00591391]\n",
      "  [0.00930072]\n",
      "  [0.02418318]\n",
      "  ...\n",
      "  [0.04342943]\n",
      "  [0.04414585]\n",
      "  [0.04655571]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.56616181]\n",
      "  [0.56129972]\n",
      "  [0.56031305]\n",
      "  ...\n",
      "  [0.66054648]\n",
      "  [0.66336019]\n",
      "  [0.6627414 ]]\n",
      "\n",
      " [[0.56129972]\n",
      "  [0.56031305]\n",
      "  [0.56122483]\n",
      "  ...\n",
      "  [0.66336019]\n",
      "  [0.6627414 ]\n",
      "  [0.66534664]]\n",
      "\n",
      " [[0.56031305]\n",
      "  [0.56122483]\n",
      "  [0.56414921]\n",
      "  ...\n",
      "  [0.6627414 ]\n",
      "  [0.66534664]\n",
      "  [0.67221148]]], the MSE is 6.291389662035164e-05\n",
      "Epoch 1/100\n",
      "204/204 [==============================] - 45s 150ms/step - loss: 0.0082 - val_loss: 7.7511e-04\n",
      "Epoch 2/100\n",
      "204/204 [==============================] - 28s 135ms/step - loss: 0.0027 - val_loss: 0.0016\n",
      "Epoch 3/100\n",
      "204/204 [==============================] - 29s 141ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 4/100\n",
      "204/204 [==============================] - 26s 129ms/step - loss: 0.0021 - val_loss: 6.0751e-04\n",
      "Epoch 5/100\n",
      "204/204 [==============================] - 27s 131ms/step - loss: 0.0019 - val_loss: 5.7644e-04\n",
      "Epoch 6/100\n",
      "204/204 [==============================] - 26s 125ms/step - loss: 0.0015 - val_loss: 6.6384e-04\n",
      "Epoch 7/100\n",
      "204/204 [==============================] - 26s 127ms/step - loss: 0.0015 - val_loss: 5.8637e-04\n",
      "Epoch 8/100\n",
      "204/204 [==============================] - 25s 124ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 9/100\n",
      "204/204 [==============================] - 26s 126ms/step - loss: 0.0013 - val_loss: 4.0634e-04\n",
      "Epoch 10/100\n",
      "204/204 [==============================] - 25s 124ms/step - loss: 0.0012 - val_loss: 5.4078e-04\n",
      "Epoch 11/100\n",
      "204/204 [==============================] - 26s 128ms/step - loss: 0.0011 - val_loss: 3.6041e-04\n",
      "Epoch 12/100\n",
      "204/204 [==============================] - 25s 125ms/step - loss: 9.7201e-04 - val_loss: 4.2200e-04\n",
      "Epoch 13/100\n",
      "204/204 [==============================] - 26s 127ms/step - loss: 9.8195e-04 - val_loss: 3.1211e-04\n",
      "Epoch 14/100\n",
      "204/204 [==============================] - 25s 120ms/step - loss: 8.8991e-04 - val_loss: 2.9427e-04\n",
      "Epoch 15/100\n",
      "204/204 [==============================] - 26s 128ms/step - loss: 8.7255e-04 - val_loss: 4.4259e-04\n",
      "Epoch 16/100\n",
      "204/204 [==============================] - 25s 122ms/step - loss: 8.1749e-04 - val_loss: 2.6964e-04\n",
      "Epoch 17/100\n",
      "204/204 [==============================] - 25s 121ms/step - loss: 7.5110e-04 - val_loss: 7.5774e-04\n",
      "Epoch 18/100\n",
      "204/204 [==============================] - 25s 123ms/step - loss: 7.4744e-04 - val_loss: 2.8895e-04\n",
      "Epoch 19/100\n",
      "204/204 [==============================] - 25s 125ms/step - loss: 7.0126e-04 - val_loss: 4.2283e-04\n",
      "Epoch 20/100\n",
      "204/204 [==============================] - 25s 124ms/step - loss: 7.5408e-04 - val_loss: 2.2224e-04\n",
      "Epoch 21/100\n",
      "204/204 [==============================] - 25s 122ms/step - loss: 6.7697e-04 - val_loss: 4.2488e-04\n",
      "Epoch 22/100\n",
      "204/204 [==============================] - 25s 123ms/step - loss: 6.6653e-04 - val_loss: 6.0068e-04\n",
      "Epoch 23/100\n",
      "204/204 [==============================] - 25s 122ms/step - loss: 6.4510e-04 - val_loss: 2.8114e-04\n",
      "Epoch 24/100\n",
      "204/204 [==============================] - 26s 127ms/step - loss: 6.3563e-04 - val_loss: 2.2681e-04\n",
      "Epoch 25/100\n",
      "204/204 [==============================] - 26s 125ms/step - loss: 5.9933e-04 - val_loss: 2.1739e-04\n",
      "Epoch 26/100\n",
      "204/204 [==============================] - 25s 125ms/step - loss: 6.1415e-04 - val_loss: 3.7092e-04\n",
      "Epoch 27/100\n",
      "204/204 [==============================] - 25s 124ms/step - loss: 5.7811e-04 - val_loss: 3.2891e-04\n",
      "Epoch 28/100\n",
      "204/204 [==============================] - 25s 124ms/step - loss: 6.1614e-04 - val_loss: 1.5954e-04\n",
      "Epoch 29/100\n",
      "204/204 [==============================] - 26s 126ms/step - loss: 5.8457e-04 - val_loss: 3.1602e-04\n",
      "Epoch 30/100\n",
      "204/204 [==============================] - 26s 127ms/step - loss: 5.7083e-04 - val_loss: 2.4767e-04\n",
      "Epoch 31/100\n",
      "204/204 [==============================] - 26s 126ms/step - loss: 5.7049e-04 - val_loss: 3.7210e-04\n",
      "Epoch 32/100\n",
      "204/204 [==============================] - 25s 125ms/step - loss: 5.3449e-04 - val_loss: 4.9753e-04\n",
      "Epoch 33/100\n",
      "204/204 [==============================] - 25s 123ms/step - loss: 5.7165e-04 - val_loss: 1.7208e-04\n",
      "Epoch 34/100\n",
      "204/204 [==============================] - 25s 122ms/step - loss: 5.6525e-04 - val_loss: 2.5609e-04\n",
      "Epoch 35/100\n",
      "204/204 [==============================] - 25s 123ms/step - loss: 5.1126e-04 - val_loss: 2.6498e-04\n",
      "Epoch 36/100\n",
      "204/204 [==============================] - 25s 124ms/step - loss: 5.5072e-04 - val_loss: 4.5434e-04\n",
      "Epoch 37/100\n",
      "204/204 [==============================] - 25s 124ms/step - loss: 5.2111e-04 - val_loss: 2.0708e-04\n",
      "Epoch 38/100\n",
      "204/204 [==============================] - 26s 127ms/step - loss: 5.3039e-04 - val_loss: 1.2833e-04\n",
      "Epoch 39/100\n",
      "204/204 [==============================] - 25s 121ms/step - loss: 5.3685e-04 - val_loss: 1.3562e-04\n",
      "Epoch 40/100\n",
      "204/204 [==============================] - 25s 120ms/step - loss: 5.0442e-04 - val_loss: 1.1221e-04\n",
      "Epoch 41/100\n",
      "204/204 [==============================] - 24s 119ms/step - loss: 5.2293e-04 - val_loss: 1.5082e-04\n",
      "Epoch 42/100\n",
      "204/204 [==============================] - 25s 121ms/step - loss: 5.0828e-04 - val_loss: 1.7154e-04\n",
      "Epoch 43/100\n",
      "204/204 [==============================] - 25s 121ms/step - loss: 5.2826e-04 - val_loss: 3.7207e-04\n",
      "Epoch 44/100\n",
      "204/204 [==============================] - 25s 120ms/step - loss: 5.2457e-04 - val_loss: 2.7755e-04\n",
      "Epoch 45/100\n",
      "204/204 [==============================] - 24s 120ms/step - loss: 5.4371e-04 - val_loss: 1.6951e-04\n",
      "Epoch 46/100\n",
      "204/204 [==============================] - 24s 120ms/step - loss: 5.1850e-04 - val_loss: 1.0337e-04\n",
      "Epoch 47/100\n",
      "204/204 [==============================] - 24s 120ms/step - loss: 4.8098e-04 - val_loss: 1.2064e-04\n",
      "Epoch 48/100\n",
      "204/204 [==============================] - 24s 120ms/step - loss: 5.1576e-04 - val_loss: 3.8340e-04\n",
      "Epoch 49/100\n",
      "204/204 [==============================] - 24s 120ms/step - loss: 4.8987e-04 - val_loss: 6.2364e-04\n",
      "Epoch 50/100\n",
      "204/204 [==============================] - 25s 121ms/step - loss: 4.8827e-04 - val_loss: 1.0102e-04\n",
      "Epoch 51/100\n",
      "204/204 [==============================] - 24s 119ms/step - loss: 4.8960e-04 - val_loss: 1.1094e-04\n",
      "Epoch 52/100\n",
      "204/204 [==============================] - 24s 120ms/step - loss: 4.9044e-04 - val_loss: 1.5911e-04\n",
      "Epoch 53/100\n",
      "204/204 [==============================] - 23s 113ms/step - loss: 4.7139e-04 - val_loss: 2.8106e-04\n",
      "Epoch 54/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.8657e-04 - val_loss: 1.1160e-04\n",
      "Epoch 55/100\n",
      "204/204 [==============================] - 24s 117ms/step - loss: 5.0180e-04 - val_loss: 3.5748e-04\n",
      "Epoch 56/100\n",
      "204/204 [==============================] - 24s 120ms/step - loss: 4.9434e-04 - val_loss: 0.0014\n",
      "Epoch 57/100\n",
      "204/204 [==============================] - 24s 119ms/step - loss: 5.0235e-04 - val_loss: 2.0138e-04\n",
      "Epoch 58/100\n",
      "204/204 [==============================] - 24s 119ms/step - loss: 4.6747e-04 - val_loss: 9.2787e-04\n",
      "Epoch 59/100\n",
      "204/204 [==============================] - 24s 119ms/step - loss: 4.6621e-04 - val_loss: 2.0892e-04\n",
      "Epoch 60/100\n",
      "204/204 [==============================] - 24s 119ms/step - loss: 4.9075e-04 - val_loss: 9.6113e-05\n",
      "Epoch 61/100\n",
      "204/204 [==============================] - 21s 105ms/step - loss: 4.8152e-04 - val_loss: 1.2509e-04\n",
      "Epoch 62/100\n",
      "204/204 [==============================] - 23s 111ms/step - loss: 4.6138e-04 - val_loss: 7.6682e-04\n",
      "Epoch 63/100\n",
      "204/204 [==============================] - 22s 110ms/step - loss: 4.6306e-04 - val_loss: 1.0715e-04\n",
      "Epoch 64/100\n",
      "204/204 [==============================] - 23s 111ms/step - loss: 4.5729e-04 - val_loss: 9.7183e-05\n",
      "Epoch 65/100\n",
      "204/204 [==============================] - 23s 111ms/step - loss: 4.7798e-04 - val_loss: 8.9396e-05\n",
      "Epoch 66/100\n",
      "204/204 [==============================] - 24s 117ms/step - loss: 4.5850e-04 - val_loss: 5.1957e-04\n",
      "Epoch 67/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.6119e-04 - val_loss: 1.7292e-04\n",
      "Epoch 68/100\n",
      "204/204 [==============================] - 24s 117ms/step - loss: 5.0846e-04 - val_loss: 2.9042e-04\n",
      "Epoch 69/100\n",
      "204/204 [==============================] - 24s 117ms/step - loss: 4.5259e-04 - val_loss: 1.8510e-04\n",
      "Epoch 70/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.5622e-04 - val_loss: 9.0340e-05\n",
      "Epoch 71/100\n",
      "204/204 [==============================] - 24s 117ms/step - loss: 4.3862e-04 - val_loss: 1.2526e-04\n",
      "Epoch 72/100\n",
      "204/204 [==============================] - 23s 115ms/step - loss: 4.6177e-04 - val_loss: 3.1002e-04\n",
      "Epoch 73/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.3733e-04 - val_loss: 1.2529e-04\n",
      "Epoch 74/100\n",
      "204/204 [==============================] - 24s 115ms/step - loss: 4.5744e-04 - val_loss: 8.9754e-05\n",
      "Epoch 75/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.5015e-04 - val_loss: 8.6883e-05\n",
      "Epoch 76/100\n",
      "204/204 [==============================] - 23s 114ms/step - loss: 4.2856e-04 - val_loss: 1.0452e-04\n",
      "Epoch 77/100\n",
      "204/204 [==============================] - 24s 117ms/step - loss: 4.6091e-04 - val_loss: 5.3874e-04\n",
      "Epoch 78/100\n",
      "204/204 [==============================] - 24s 117ms/step - loss: 4.5475e-04 - val_loss: 2.0576e-04\n",
      "Epoch 79/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.3677e-04 - val_loss: 8.9170e-05\n",
      "Epoch 80/100\n",
      "204/204 [==============================] - 24s 117ms/step - loss: 4.5731e-04 - val_loss: 9.7808e-05\n",
      "Epoch 81/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.3127e-04 - val_loss: 1.2399e-04\n",
      "Epoch 82/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.3033e-04 - val_loss: 1.6859e-04\n",
      "Epoch 83/100\n",
      "204/204 [==============================] - 23s 115ms/step - loss: 4.1870e-04 - val_loss: 8.6735e-05\n",
      "Epoch 84/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.3210e-04 - val_loss: 1.0865e-04\n",
      "Epoch 85/100\n",
      "204/204 [==============================] - 23s 115ms/step - loss: 4.5318e-04 - val_loss: 1.1387e-04\n",
      "Epoch 86/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.5731e-04 - val_loss: 4.5843e-04\n",
      "Epoch 87/100\n",
      "204/204 [==============================] - 25s 121ms/step - loss: 4.6118e-04 - val_loss: 1.8814e-04\n",
      "Epoch 88/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.5324e-04 - val_loss: 2.8597e-04\n",
      "Epoch 89/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.6143e-04 - val_loss: 8.6063e-05\n",
      "Epoch 90/100\n",
      "204/204 [==============================] - 23s 115ms/step - loss: 4.0297e-04 - val_loss: 4.6193e-04\n",
      "Epoch 91/100\n",
      "204/204 [==============================] - 24s 115ms/step - loss: 4.1813e-04 - val_loss: 1.3386e-04\n",
      "Epoch 92/100\n",
      "204/204 [==============================] - 23s 115ms/step - loss: 4.4368e-04 - val_loss: 1.0969e-04\n",
      "Epoch 93/100\n",
      "204/204 [==============================] - 23s 115ms/step - loss: 4.4265e-04 - val_loss: 9.6527e-05\n",
      "Epoch 94/100\n",
      "204/204 [==============================] - 23s 115ms/step - loss: 4.4990e-04 - val_loss: 1.3319e-04\n",
      "Epoch 95/100\n",
      "204/204 [==============================] - 23s 114ms/step - loss: 4.4441e-04 - val_loss: 2.3486e-04\n",
      "Epoch 96/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.4085e-04 - val_loss: 2.5117e-04\n",
      "Epoch 97/100\n",
      "204/204 [==============================] - 23s 115ms/step - loss: 4.3348e-04 - val_loss: 1.5388e-04\n",
      "Epoch 98/100\n",
      "204/204 [==============================] - 24s 116ms/step - loss: 4.2654e-04 - val_loss: 7.7021e-05\n",
      "Epoch 99/100\n",
      "204/204 [==============================] - 24s 117ms/step - loss: 4.2556e-04 - val_loss: 2.5896e-04\n",
      "Epoch 100/100\n",
      "204/204 [==============================] - 24s 115ms/step - loss: 4.3292e-04 - val_loss: 8.0914e-05\n",
      "16/16 [==============================] - 3s 38ms/step\n",
      "For train: [[[0.0063047 ]\n",
      "  [0.00190836]\n",
      "  [0.00591391]\n",
      "  ...\n",
      "  [0.04916096]\n",
      "  [0.04948661]\n",
      "  [0.05352475]]\n",
      "\n",
      " [[0.00190836]\n",
      "  [0.00591391]\n",
      "  [0.00930072]\n",
      "  ...\n",
      "  [0.04948661]\n",
      "  [0.05352475]\n",
      "  [0.05603228]]\n",
      "\n",
      " [[0.00591391]\n",
      "  [0.00930072]\n",
      "  [0.02418318]\n",
      "  ...\n",
      "  [0.05352475]\n",
      "  [0.05603228]\n",
      "  [0.05808392]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.56679357]\n",
      "  [0.56635064]\n",
      "  [0.57255439]\n",
      "  ...\n",
      "  [0.66054648]\n",
      "  [0.66336019]\n",
      "  [0.6627414 ]]\n",
      "\n",
      " [[0.56635064]\n",
      "  [0.57255439]\n",
      "  [0.58322613]\n",
      "  ...\n",
      "  [0.66336019]\n",
      "  [0.6627414 ]\n",
      "  [0.66534664]]\n",
      "\n",
      " [[0.57255439]\n",
      "  [0.58322613]\n",
      "  [0.57845531]\n",
      "  ...\n",
      "  [0.6627414 ]\n",
      "  [0.66534664]\n",
      "  [0.67221148]]], the MSE is 6.408796432226227e-05\n",
      "Epoch 1/100\n",
      "201/201 [==============================] - 60s 244ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 2/100\n",
      "201/201 [==============================] - 42s 211ms/step - loss: 0.0024 - val_loss: 8.8481e-04\n",
      "Epoch 3/100\n",
      "201/201 [==============================] - 42s 211ms/step - loss: 0.0022 - val_loss: 7.8409e-04\n",
      "Epoch 4/100\n",
      "201/201 [==============================] - 43s 213ms/step - loss: 0.0019 - val_loss: 8.5822e-04\n",
      "Epoch 5/100\n",
      "201/201 [==============================] - 42s 211ms/step - loss: 0.0018 - val_loss: 9.3663e-04\n",
      "Epoch 6/100\n",
      "201/201 [==============================] - 43s 212ms/step - loss: 0.0017 - val_loss: 5.2317e-04\n",
      "Epoch 7/100\n",
      "201/201 [==============================] - 42s 211ms/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 8/100\n",
      "201/201 [==============================] - 43s 212ms/step - loss: 0.0014 - val_loss: 5.3321e-04\n",
      "Epoch 9/100\n",
      "201/201 [==============================] - 42s 211ms/step - loss: 0.0012 - val_loss: 5.4494e-04\n",
      "Epoch 10/100\n",
      "201/201 [==============================] - 42s 211ms/step - loss: 0.0012 - val_loss: 4.0259e-04\n",
      "Epoch 11/100\n",
      "201/201 [==============================] - 42s 210ms/step - loss: 0.0011 - val_loss: 4.3450e-04\n",
      "Epoch 12/100\n",
      "201/201 [==============================] - 42s 211ms/step - loss: 0.0010 - val_loss: 3.4992e-04\n",
      "Epoch 13/100\n",
      "201/201 [==============================] - 43s 212ms/step - loss: 9.3026e-04 - val_loss: 4.8188e-04\n",
      "Epoch 14/100\n",
      "201/201 [==============================] - 42s 211ms/step - loss: 9.2820e-04 - val_loss: 3.1777e-04\n",
      "Epoch 15/100\n",
      "201/201 [==============================] - 43s 212ms/step - loss: 8.6764e-04 - val_loss: 2.8770e-04\n",
      "Epoch 16/100\n",
      "201/201 [==============================] - 43s 212ms/step - loss: 8.7137e-04 - val_loss: 4.5785e-04\n",
      "Epoch 17/100\n",
      "201/201 [==============================] - 43s 212ms/step - loss: 7.7403e-04 - val_loss: 5.4578e-04\n",
      "Epoch 18/100\n",
      "201/201 [==============================] - 43s 212ms/step - loss: 7.8799e-04 - val_loss: 5.8838e-04\n",
      "Epoch 19/100\n",
      "201/201 [==============================] - 43s 212ms/step - loss: 7.3862e-04 - val_loss: 2.5787e-04\n",
      "Epoch 20/100\n",
      "201/201 [==============================] - 43s 212ms/step - loss: 7.3078e-04 - val_loss: 2.5258e-04\n",
      "Epoch 21/100\n",
      "201/201 [==============================] - 43s 212ms/step - loss: 6.8244e-04 - val_loss: 0.0017\n",
      "Epoch 22/100\n",
      "201/201 [==============================] - 42s 211ms/step - loss: 6.8202e-04 - val_loss: 3.2284e-04\n",
      "Epoch 23/100\n",
      "201/201 [==============================] - 42s 211ms/step - loss: 6.6326e-04 - val_loss: 2.5779e-04\n",
      "Epoch 24/100\n",
      "201/201 [==============================] - 43s 213ms/step - loss: 6.5872e-04 - val_loss: 2.2271e-04\n",
      "Epoch 25/100\n",
      "201/201 [==============================] - 43s 212ms/step - loss: 6.2840e-04 - val_loss: 2.1301e-04\n",
      "Epoch 26/100\n",
      "201/201 [==============================] - 43s 215ms/step - loss: 5.7421e-04 - val_loss: 2.0896e-04\n",
      "Epoch 27/100\n",
      "201/201 [==============================] - 43s 213ms/step - loss: 6.2496e-04 - val_loss: 2.0998e-04\n",
      "Epoch 28/100\n",
      "201/201 [==============================] - 42s 210ms/step - loss: 6.2756e-04 - val_loss: 4.7117e-04\n",
      "Epoch 29/100\n",
      "201/201 [==============================] - 42s 209ms/step - loss: 6.0368e-04 - val_loss: 1.8464e-04\n",
      "Epoch 30/100\n",
      "201/201 [==============================] - 42s 209ms/step - loss: 5.7320e-04 - val_loss: 1.7299e-04\n",
      "Epoch 31/100\n",
      "201/201 [==============================] - 42s 211ms/step - loss: 5.9059e-04 - val_loss: 1.6519e-04\n",
      "Epoch 32/100\n",
      "201/201 [==============================] - 43s 215ms/step - loss: 5.7844e-04 - val_loss: 8.5333e-04\n",
      "Epoch 33/100\n",
      "201/201 [==============================] - 43s 212ms/step - loss: 5.7684e-04 - val_loss: 1.8685e-04\n",
      "Epoch 34/100\n",
      "201/201 [==============================] - 42s 210ms/step - loss: 5.4464e-04 - val_loss: 1.8234e-04\n",
      "Epoch 35/100\n",
      "201/201 [==============================] - 42s 211ms/step - loss: 5.8004e-04 - val_loss: 1.6149e-04\n",
      "Epoch 36/100\n",
      "201/201 [==============================] - 42s 209ms/step - loss: 5.4435e-04 - val_loss: 2.6981e-04\n",
      "Epoch 37/100\n",
      "201/201 [==============================] - 42s 209ms/step - loss: 5.4416e-04 - val_loss: 1.8705e-04\n",
      "Epoch 38/100\n",
      "201/201 [==============================] - 41s 206ms/step - loss: 5.6683e-04 - val_loss: 5.0227e-04\n",
      "Epoch 39/100\n",
      "201/201 [==============================] - 41s 205ms/step - loss: 5.2630e-04 - val_loss: 1.3764e-04\n",
      "Epoch 40/100\n",
      "201/201 [==============================] - 41s 204ms/step - loss: 5.2700e-04 - val_loss: 2.7423e-04\n",
      "Epoch 41/100\n",
      "201/201 [==============================] - 41s 204ms/step - loss: 5.7001e-04 - val_loss: 3.2366e-04\n",
      "Epoch 42/100\n",
      "201/201 [==============================] - 41s 203ms/step - loss: 5.1817e-04 - val_loss: 1.5081e-04\n",
      "Epoch 43/100\n",
      "201/201 [==============================] - 41s 202ms/step - loss: 5.0945e-04 - val_loss: 1.3950e-04\n",
      "Epoch 44/100\n",
      "201/201 [==============================] - 41s 203ms/step - loss: 4.8465e-04 - val_loss: 1.4142e-04\n",
      "Epoch 45/100\n",
      "201/201 [==============================] - 41s 202ms/step - loss: 4.9964e-04 - val_loss: 1.8337e-04\n",
      "Epoch 46/100\n",
      "201/201 [==============================] - 41s 204ms/step - loss: 5.1658e-04 - val_loss: 1.8325e-04\n",
      "Epoch 47/100\n",
      "201/201 [==============================] - 41s 204ms/step - loss: 5.2241e-04 - val_loss: 1.4805e-04\n",
      "Epoch 48/100\n",
      "201/201 [==============================] - 41s 203ms/step - loss: 5.2302e-04 - val_loss: 1.4285e-04\n",
      "Epoch 49/100\n",
      "201/201 [==============================] - 41s 204ms/step - loss: 5.1767e-04 - val_loss: 1.8132e-04\n",
      "Epoch 50/100\n",
      "201/201 [==============================] - 41s 203ms/step - loss: 5.3093e-04 - val_loss: 1.2095e-04\n",
      "Epoch 51/100\n",
      "201/201 [==============================] - 41s 203ms/step - loss: 5.0114e-04 - val_loss: 1.1428e-04\n",
      "Epoch 52/100\n",
      "201/201 [==============================] - 41s 202ms/step - loss: 4.8934e-04 - val_loss: 1.4581e-04\n",
      "Epoch 53/100\n",
      "201/201 [==============================] - 40s 197ms/step - loss: 4.9822e-04 - val_loss: 3.3159e-04\n",
      "Epoch 54/100\n",
      "201/201 [==============================] - 40s 199ms/step - loss: 4.9025e-04 - val_loss: 1.3715e-04\n",
      "Epoch 55/100\n",
      "201/201 [==============================] - 40s 201ms/step - loss: 4.4747e-04 - val_loss: 1.1427e-04\n",
      "Epoch 56/100\n",
      "201/201 [==============================] - 40s 199ms/step - loss: 4.9482e-04 - val_loss: 1.2374e-04\n",
      "Epoch 57/100\n",
      "201/201 [==============================] - 40s 200ms/step - loss: 4.9127e-04 - val_loss: 2.2987e-04\n",
      "Epoch 58/100\n",
      "201/201 [==============================] - 40s 198ms/step - loss: 4.9285e-04 - val_loss: 8.1377e-04\n",
      "Epoch 59/100\n",
      "201/201 [==============================] - 40s 199ms/step - loss: 4.8566e-04 - val_loss: 4.1090e-04\n",
      "Epoch 60/100\n",
      "201/201 [==============================] - 40s 199ms/step - loss: 4.7462e-04 - val_loss: 1.1833e-04\n",
      "Epoch 61/100\n",
      "201/201 [==============================] - 41s 206ms/step - loss: 4.8382e-04 - val_loss: 1.5404e-04\n",
      "Epoch 62/100\n",
      "201/201 [==============================] - 40s 200ms/step - loss: 4.7889e-04 - val_loss: 1.0459e-04\n",
      "Epoch 63/100\n",
      "201/201 [==============================] - 40s 199ms/step - loss: 4.5831e-04 - val_loss: 1.0834e-04\n",
      "Epoch 64/100\n",
      "201/201 [==============================] - 40s 198ms/step - loss: 4.8032e-04 - val_loss: 1.4914e-04\n",
      "Epoch 65/100\n",
      "201/201 [==============================] - 40s 197ms/step - loss: 4.6531e-04 - val_loss: 2.4462e-04\n",
      "Epoch 66/100\n",
      "201/201 [==============================] - 40s 198ms/step - loss: 4.5115e-04 - val_loss: 1.3867e-04\n",
      "Epoch 67/100\n",
      "201/201 [==============================] - 40s 199ms/step - loss: 4.8313e-04 - val_loss: 1.5098e-04\n",
      "Epoch 68/100\n",
      "201/201 [==============================] - 41s 202ms/step - loss: 4.6120e-04 - val_loss: 1.7148e-04\n",
      "Epoch 69/100\n",
      "201/201 [==============================] - 40s 200ms/step - loss: 4.8481e-04 - val_loss: 1.7612e-04\n",
      "Epoch 70/100\n",
      "201/201 [==============================] - 40s 198ms/step - loss: 4.4986e-04 - val_loss: 1.9641e-04\n",
      "Epoch 71/100\n",
      "201/201 [==============================] - 40s 197ms/step - loss: 4.5522e-04 - val_loss: 1.1098e-04\n",
      "Epoch 72/100\n",
      "201/201 [==============================] - 39s 196ms/step - loss: 4.5206e-04 - val_loss: 1.4174e-04\n",
      "Epoch 73/100\n",
      "201/201 [==============================] - 39s 196ms/step - loss: 4.7196e-04 - val_loss: 1.1611e-04\n",
      "Epoch 74/100\n",
      "201/201 [==============================] - 40s 199ms/step - loss: 4.6107e-04 - val_loss: 2.0578e-04\n",
      "Epoch 75/100\n",
      "201/201 [==============================] - 39s 196ms/step - loss: 4.3866e-04 - val_loss: 9.3378e-05\n",
      "Epoch 76/100\n",
      "201/201 [==============================] - 40s 199ms/step - loss: 4.3989e-04 - val_loss: 9.6284e-05\n",
      "Epoch 77/100\n",
      "201/201 [==============================] - 39s 196ms/step - loss: 4.5022e-04 - val_loss: 1.0766e-04\n",
      "Epoch 78/100\n",
      "201/201 [==============================] - 39s 196ms/step - loss: 4.5766e-04 - val_loss: 9.5668e-05\n",
      "Epoch 79/100\n",
      "201/201 [==============================] - 39s 196ms/step - loss: 4.5158e-04 - val_loss: 2.3945e-04\n",
      "Epoch 80/100\n",
      "201/201 [==============================] - 38s 190ms/step - loss: 4.3038e-04 - val_loss: 1.0203e-04\n",
      "Epoch 81/100\n",
      "201/201 [==============================] - 37s 185ms/step - loss: 4.3888e-04 - val_loss: 2.1256e-04\n",
      "Epoch 82/100\n",
      "201/201 [==============================] - 37s 186ms/step - loss: 4.3547e-04 - val_loss: 1.3453e-04\n",
      "Epoch 83/100\n",
      "201/201 [==============================] - 38s 188ms/step - loss: 4.4148e-04 - val_loss: 1.7476e-04\n",
      "Epoch 84/100\n",
      "201/201 [==============================] - 39s 193ms/step - loss: 4.3205e-04 - val_loss: 4.2491e-04\n",
      "Epoch 85/100\n",
      "201/201 [==============================] - 38s 187ms/step - loss: 4.4188e-04 - val_loss: 1.0873e-04\n",
      "Epoch 86/100\n",
      "201/201 [==============================] - 38s 188ms/step - loss: 4.2475e-04 - val_loss: 3.0509e-04\n",
      "Epoch 87/100\n",
      "201/201 [==============================] - 39s 192ms/step - loss: 4.3809e-04 - val_loss: 9.0534e-05\n",
      "Epoch 88/100\n",
      "201/201 [==============================] - 39s 192ms/step - loss: 4.6226e-04 - val_loss: 1.1676e-04\n",
      "Epoch 89/100\n",
      "201/201 [==============================] - 37s 184ms/step - loss: 4.4945e-04 - val_loss: 9.0943e-05\n",
      "Epoch 90/100\n",
      "201/201 [==============================] - 37s 185ms/step - loss: 4.3507e-04 - val_loss: 1.5453e-04\n",
      "Epoch 91/100\n",
      "201/201 [==============================] - 38s 189ms/step - loss: 4.1094e-04 - val_loss: 3.1749e-04\n",
      "Epoch 92/100\n",
      "201/201 [==============================] - 38s 189ms/step - loss: 4.4780e-04 - val_loss: 9.0456e-05\n",
      "Epoch 93/100\n",
      "201/201 [==============================] - 37s 184ms/step - loss: 4.3641e-04 - val_loss: 8.3749e-05\n",
      "Epoch 94/100\n",
      "201/201 [==============================] - 38s 188ms/step - loss: 4.4099e-04 - val_loss: 1.9319e-04\n",
      "Epoch 95/100\n",
      "201/201 [==============================] - 37s 186ms/step - loss: 4.3459e-04 - val_loss: 1.5418e-04\n",
      "Epoch 96/100\n",
      "201/201 [==============================] - 39s 196ms/step - loss: 4.3326e-04 - val_loss: 8.9370e-05\n",
      "Epoch 97/100\n",
      "201/201 [==============================] - 38s 189ms/step - loss: 4.3870e-04 - val_loss: 9.9779e-05\n",
      "Epoch 98/100\n",
      "201/201 [==============================] - 38s 188ms/step - loss: 4.5763e-04 - val_loss: 9.7393e-05\n",
      "Epoch 99/100\n",
      "201/201 [==============================] - 38s 188ms/step - loss: 4.3534e-04 - val_loss: 1.1652e-04\n",
      "Epoch 100/100\n",
      "201/201 [==============================] - 38s 188ms/step - loss: 4.2476e-04 - val_loss: 1.0575e-04\n",
      "16/16 [==============================] - 3s 67ms/step\n",
      "For train: [[[0.0063047 ]\n",
      "  [0.00190836]\n",
      "  [0.00591391]\n",
      "  ...\n",
      "  [0.07075191]\n",
      "  [0.07599497]\n",
      "  [0.07798146]]\n",
      "\n",
      " [[0.00190836]\n",
      "  [0.00591391]\n",
      "  [0.00930072]\n",
      "  ...\n",
      "  [0.07599497]\n",
      "  [0.07798146]\n",
      "  [0.07544133]]\n",
      "\n",
      " [[0.00591391]\n",
      "  [0.00930072]\n",
      "  [0.02418318]\n",
      "  ...\n",
      "  [0.07798146]\n",
      "  [0.07544133]\n",
      "  [0.0775581 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.66026646]\n",
      "  [0.61014804]\n",
      "  [0.55059547]\n",
      "  ...\n",
      "  [0.66054648]\n",
      "  [0.66336019]\n",
      "  [0.6627414 ]]\n",
      "\n",
      " [[0.61014804]\n",
      "  [0.55059547]\n",
      "  [0.53716544]\n",
      "  ...\n",
      "  [0.66336019]\n",
      "  [0.6627414 ]\n",
      "  [0.66534664]]\n",
      "\n",
      " [[0.55059547]\n",
      "  [0.53716544]\n",
      "  [0.52775077]\n",
      "  ...\n",
      "  [0.6627414 ]\n",
      "  [0.66534664]\n",
      "  [0.67221148]]], the MSE is 6.977327030692196e-05\n"
     ]
    }
   ],
   "source": [
    "# Initialise RNN\n",
    "for j,k,l,m in zip(x_train_list, y_train_list, x_test_list, y_test_list):\n",
    "    LSTM_regressor = Sequential()\n",
    "    # Add LSTM layers and dropout regularization\n",
    "    # Units: Number of neurons in the hidden layer\n",
    "    # Activation: Activation function to be used, ReLu, sigmoid or tanh\n",
    "    # Input_shape: Input shape to be provided to the LSTM RNN\n",
    "    LSTM_regressor.add(LSTM (units= 50, return_sequences = True, input_shape = (j.shape[1], 1))) # Adding 50 neurons\n",
    "    LSTM_regressor.add(Dropout(0.2))\n",
    "\n",
    "    # Adding a second LSTM layer and some Dropout regularisation\n",
    "    LSTM_regressor.add(LSTM(units= 50, return_sequences = True)) # Input has to be provided to first layer only, return_sequence=True help to access hidden state output for every layer \n",
    "    LSTM_regressor.add(Dropout(0.2))\n",
    "\n",
    "    # Adding a third LSTM layer and some Dropout regularisation\n",
    "    LSTM_regressor.add(LSTM(units= 50))\n",
    "    LSTM_regressor.add(Dropout(0.2))\n",
    "\n",
    "    # Adding the output layer\n",
    "    LSTM_regressor.add(Dense(units=1))\n",
    "    \n",
    "    # Compile models\n",
    "    LSTM_regressor.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "    # Fitting RNN to training set\n",
    "    history = LSTM_regressor.fit(j, k, validation_split=0.2, epochs=100, batch_size=32)\n",
    "    \n",
    "    prediction = LSTM_regressor.predict(l)\n",
    "\n",
    "    mse = metrics.mean_squared_error(prediction, m)\n",
    "    print(f\"For train: {j}, the MSE is {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mages",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81e19f9b470b02458b24c42aaa76c128fc4f599a17bf0001519e13778e9988da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
